#!/bin/bash
# vLLM Model Management Script
# ===========================

PROJECT_ROOT="/home/chanthaphan/project"
VENV_PATH="$PROJECT_ROOT/llm-env/bin/python"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

print_color() {
    echo -e "${1}${2}${NC}"
}

print_header() {
    echo ""
    print_color $BLUE "================================================"
    print_color $BLUE "vLLM Model Management"
    print_color $BLUE "================================================"
    echo ""
}

show_model_status() {
    print_color $GREEN "üìä Current Model Status:"
    echo ""
    
    # Check available models
    if [ -d "models/qwen_thai_merged" ]; then
        print_color $GREEN "‚úÖ Thai Merged Model: models/qwen_thai_merged"
        MODEL_SIZE=$(du -sh models/qwen_thai_merged | cut -f1)
        echo "   üì¶ Size: $MODEL_SIZE"
    else
        print_color $YELLOW "‚ùå Thai Merged Model: Not found"
    fi
    
    if [ -d "models/qwen_thai_lora" ]; then
        print_color $GREEN "‚úÖ Thai LoRA Adapter: models/qwen_thai_lora"
        LORA_SIZE=$(du -sh models/qwen_thai_lora | cut -f1)
        echo "   üì¶ Size: $LORA_SIZE"
    else
        print_color $YELLOW "‚ùå Thai LoRA Adapter: Not found"
    fi
    
    # Check if vLLM server is running
    if pgrep -f "vllm.entrypoints.openai.api_server" > /dev/null; then
        print_color $GREEN "üöÄ vLLM Server: Running"
        VLLM_PID=$(pgrep -f "vllm.entrypoints.openai.api_server")
        echo "   üî¢ PID: $VLLM_PID"
        echo "   üåê Endpoint: http://localhost:8000"
    else
        print_color $YELLOW "‚èπÔ∏è  vLLM Server: Not running"
    fi
    
    echo ""
}

start_vllm() {
    print_color $BLUE "üöÄ Starting vLLM server..."
    
    # Check if already running
    if pgrep -f "vllm.entrypoints.openai.api_server" > /dev/null; then
        print_color $YELLOW "‚ö†Ô∏è  vLLM server is already running!"
        print_color $BLUE "üí° Use 'stop' command to stop it first"
        return 1
    fi
    
    cd "$PROJECT_ROOT"
    
    # Determine which model to use
    if [ -d "models/qwen_thai_merged" ]; then
        MODEL_PATH="models/qwen_thai_merged"
        print_color $GREEN "üì¶ Using Thai merged model: $MODEL_PATH"
    else
        MODEL_PATH="Qwen/Qwen2.5-1.5B-Instruct"
        print_color $YELLOW "‚ö†Ô∏è  Using base model: $MODEL_PATH"
        print_color $YELLOW "üí° Merge LoRA first for Thai capabilities: ./manage.sh merge-model"
    fi
    
    print_color $BLUE "‚è≥ Loading model (this may take a few minutes)..."
    
    # Start vLLM server in background with custom parameters
    nohup $VENV_PATH -m vllm.entrypoints.openai.api_server \
        --model "$MODEL_PATH" \
        --host 0.0.0.0 \
        --port 8000 \
        --served-model-name thai-model \
        --max-model-len 4096 \
        --gpu-memory-utilization 0.8 \
        --dtype float16 > logs/vllm_server.log 2>&1 &
    
    # Wait a moment and check if it started
    sleep 5
    if pgrep -f "vllm.entrypoints.openai.api_server" > /dev/null; then
        print_color $GREEN "‚úÖ vLLM server started successfully!"
        print_color $BLUE "üåê API endpoint: http://localhost:8000"
        print_color $BLUE "üìö Documentation: http://localhost:8000/docs"
        print_color $BLUE "üìã Logs: tail -f logs/vllm_server.log"
    else
        print_color $RED "‚ùå Failed to start vLLM server"
        print_color $BLUE "üí° Check logs: cat logs/vllm_server.log"
    fi
}

stop_vllm() {
    print_color $BLUE "‚èπÔ∏è  Stopping vLLM server..."
    
    VLLM_PID=$(pgrep -f "vllm.entrypoints.openai.api_server")
    if [ -n "$VLLM_PID" ]; then
        kill $VLLM_PID
        sleep 3
        
        # Force kill if still running
        if pgrep -f "vllm.entrypoints.openai.api_server" > /dev/null; then
            print_color $YELLOW "‚ö†Ô∏è  Force stopping..."
            pkill -9 -f "vllm.entrypoints.openai.api_server"
        fi
        
        print_color $GREEN "‚úÖ vLLM server stopped"
    else
        print_color $YELLOW "‚ö†Ô∏è  vLLM server is not running"
    fi
}

restart_vllm() {
    print_color $BLUE "üîÑ Restarting vLLM server..."
    stop_vllm
    sleep 2
    start_vllm
}

test_vllm() {
    print_color $BLUE "üß™ Testing vLLM server..."
    
    if ! pgrep -f "vllm.entrypoints.openai.api_server" > /dev/null; then
        print_color $RED "‚ùå vLLM server is not running"
        print_color $BLUE "üí° Start it first: $0 start"
        return 1
    fi
    
    # Test health endpoint
    print_color $YELLOW "üì° Testing health endpoint..."
    if curl -s http://localhost:8000/health > /dev/null; then
        print_color $GREEN "‚úÖ Health check passed"
    else
        print_color $RED "‚ùå Health check failed"
        return 1
    fi
    
    # Test models endpoint
    print_color $YELLOW "üìã Available models:"
    curl -s http://localhost:8000/v1/models | jq '.data[0].id' || echo "thai-model"
    
    # Test chat completion
    print_color $YELLOW "üí¨ Testing chat completion..."
    RESPONSE=$(curl -s -X POST http://localhost:8000/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
            "model": "thai-model",
            "messages": [{"role": "user", "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ"}],
            "max_tokens": 50,
            "stream": false
        }')
    
    if echo "$RESPONSE" | jq -e '.choices[0].message.content' > /dev/null 2>&1; then
        print_color $GREEN "‚úÖ Chat completion test passed"
        echo "üìù Response: $(echo "$RESPONSE" | jq -r '.choices[0].message.content')"
    else
        print_color $RED "‚ùå Chat completion test failed"
        echo "üìù Response: $RESPONSE"
    fi
}

show_logs() {
    print_color $BLUE "üìã vLLM Server Logs:"
    echo ""
    
    if [ -f "logs/vllm_server.log" ]; then
        tail -50 logs/vllm_server.log
    else
        print_color $YELLOW "‚ö†Ô∏è  No log file found: logs/vllm_server.log"
    fi
}

configure_model() {
    print_color $BLUE "‚öôÔ∏è  Model Configuration Options:"
    echo ""
    echo "1. üéØ Model Selection:"
    echo "   ‚Ä¢ Base Model: Qwen/Qwen2.5-1.5B-Instruct"
    echo "   ‚Ä¢ Thai Model: models/qwen_thai_merged (recommended)"
    echo ""
    echo "2. üîß Performance Tuning:"
    echo "   ‚Ä¢ GPU Memory: --gpu-memory-utilization 0.8"
    echo "   ‚Ä¢ Max Length: --max-model-len 4096"
    echo "   ‚Ä¢ Data Type: --dtype float16"
    echo ""
    echo "3. üöÄ Advanced Options:"
    echo "   ‚Ä¢ Tensor Parallel: --tensor-parallel-size 1"
    echo "   ‚Ä¢ Pipeline Parallel: --pipeline-parallel-size 1"
    echo "   ‚Ä¢ Quantization: --quantization awq|gptq"
    echo ""
    
    read -p "üîß Edit vLLM configuration? (y/N): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        nano "$0"  # Edit this script
    fi
}

show_usage() {
    echo "Usage: $0 [COMMAND]"
    echo ""
    echo "Commands:"
    echo "  status      Show current model and server status"
    echo "  start       Start vLLM server"
    echo "  stop        Stop vLLM server"
    echo "  restart     Restart vLLM server"
    echo "  test        Test vLLM server functionality"
    echo "  logs        Show server logs"
    echo "  config      Configure model parameters"
    echo "  help        Show this help message"
    echo ""
    echo "Examples:"
    echo "  $0 status         # Check if server is running"
    echo "  $0 start          # Start the server"
    echo "  $0 test           # Test API endpoints"
    echo "  $0 logs           # View recent logs"
    echo ""
}

# Main script logic
case "${1:-status}" in
    status)
        print_header
        show_model_status
        ;;
    start)
        print_header
        start_vllm
        ;;
    stop)
        print_header
        stop_vllm
        ;;
    restart)
        print_header
        restart_vllm
        ;;
    test)
        print_header
        test_vllm
        ;;
    logs)
        print_header
        show_logs
        ;;
    config)
        print_header
        configure_model
        ;;
    help|--help|-h)
        print_header
        show_usage
        ;;
    *)
        print_color $RED "‚ùå Unknown command: $1"
        echo ""
        show_usage
        exit 1
        ;;
esac