# ğŸ‡¹ğŸ‡­ Thai Language Model - Restructured

**A comprehensive, production-ready Thai language model package with fine-tuning, hosting, and inference capabilities.**

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/Docker-Supported-blue.svg)](deployment/docker/)

## ğŸŒŸ Features

### ğŸ§  **AI & Machine Learning**
- **Fine-tuned Thai Language Model** based on Qwen2.5-1.5B-Instruct
- **LoRA (Low-Rank Adaptation)** for efficient fine-tuning
- **Thai text summarization** with specialized tokenization
- **Multi-modal inference** supporting various generation tasks

### ğŸš€ **Production-Ready API**
- **FastAPI server** with OpenAI-compatible endpoints
- **Streaming responses** for real-time generation
- **Automatic documentation** with Swagger/OpenAPI
- **Health monitoring** and metrics collection

### ğŸ–¥ï¸ **User Interfaces**
- **Command-line interface** for quick interactions
- **Web GUI** with Gradio for browser-based usage
- **REST API** for programmatic integration
- **Chat interfaces** for conversational AI

### ğŸ³ **DevOps & Deployment**
- **Docker containerization** with multi-stage builds
- **Kubernetes manifests** for scalable deployment
- **Nginx reverse proxy** configuration
- **Monitoring stack** with Prometheus and Grafana

## ğŸ“‹ Table of Contents

1. [Quick Start](#-quick-start)
2. [Installation](#-installation) 
3. [Project Structure](#-project-structure)
4. [Usage Examples](#-usage-examples)
5. [Configuration](#-configuration)
6. [API Documentation](#-api-documentation)
7. [Deployment](#-deployment)
8. [Development](#-development)
9. [Contributing](#-contributing)

## ğŸš€ Quick Start

### **1. Installation**

```bash
# Clone repository
git clone https://github.com/username/thai-language-model.git
cd thai-language-model

# Install package
pip install -e .

# Or install with development dependencies
pip install -e ".[dev]"
```

### **2. Run API Server**

```bash
# Using the package
thai-model-api

# Or using script directly
python scripts/api_server.py --port 8001
```

### **3. Test the API**

```bash
# Health check
curl http://localhost:8001/health

# Chat completion
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "thai-model",
    "messages": [{"role": "user", "content": "à¸ªà¸§à¸±à¸ªà¸”à¸µ"}],
    "max_tokens": 50
  }'
```

### **4. Web Interface**

```bash
# Start web GUI
python scripts/chat_web.py
```

Visit `http://localhost:7860` for the web interface.

## ğŸ“¦ Installation

### **Prerequisites**

- Python 3.8 or higher
- CUDA-compatible GPU (optional, for faster inference)
- Docker (optional, for containerized deployment)

### **Install from Source**

```bash
# Clone repository
git clone https://github.com/username/thai-language-model.git
cd thai-language-model

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install package
pip install -e .
```

### **Install with Docker**

```bash
# Build Docker image
cd deployment/docker
docker build -f Dockerfile.cpu -t thai-model-api .

# Run container
docker run -d -p 8001:8001 thai-model-api
```

### **GPU Support**

For GPU acceleration, ensure you have:

```bash
# Install GPU version of PyTorch
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# For Docker GPU support
# Use Dockerfile instead of Dockerfile.cpu
docker build -t thai-model-api .
```

## ğŸ—ï¸ Project Structure

```
thai-language-model/
â”œâ”€â”€ ğŸ“„ README.md                          # This file
â”œâ”€â”€ ğŸ“„ pyproject.toml                     # Modern Python project configuration
â”œâ”€â”€ ğŸ“„ requirements.txt                   # Python dependencies
â”œâ”€â”€ ğŸ“„ LICENSE                            # MIT License
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ thai_model/                        # Main Python package
â”‚   â”œâ”€â”€ __init__.py                       # Package initialization
â”‚   â”œâ”€â”€ core/                             # Core model functionality
â”‚   â”‚   â”œâ”€â”€ model.py                      # ThaiModel class
â”‚   â”‚   â”œâ”€â”€ config.py                     # Configuration classes
â”‚   â”‚   â””â”€â”€ tokenizer.py                  # Thai tokenization utilities
â”‚   â”œâ”€â”€ api/                              # REST API server
â”‚   â”‚   â”œâ”€â”€ fastapi_server.py             # FastAPI application
â”‚   â”‚   â”œâ”€â”€ models.py                     # Pydantic request/response models
â”‚   â”‚   â””â”€â”€ routes/                       # API route definitions
â”‚   â”œâ”€â”€ interfaces/                       # User interfaces
â”‚   â”‚   â”œâ”€â”€ cli.py                        # Command-line interface
â”‚   â”‚   â”œâ”€â”€ web.py                        # Web GUI interface
â”‚   â”‚   â””â”€â”€ chat.py                       # Chat functionality
â”‚   â”œâ”€â”€ training/                         # Training pipeline
â”‚   â”‚   â”œâ”€â”€ trainer.py                    # Model training logic
â”‚   â”‚   â”œâ”€â”€ data_loader.py                # Dataset handling
â”‚   â”‚   â””â”€â”€ evaluation.py                 # Model evaluation
â”‚   â”œâ”€â”€ utils/                            # Utilities
â”‚   â”‚   â”œâ”€â”€ logger.py                     # Logging configuration
â”‚   â”‚   â”œâ”€â”€ helpers.py                    # Helper functions
â”‚   â”‚   â””â”€â”€ validation.py                 # Input validation
â”‚   â””â”€â”€ tests/                            # Test suite
â”‚       â”œâ”€â”€ test_core.py                  # Core functionality tests
â”‚       â”œâ”€â”€ test_api.py                   # API endpoint tests
â”‚       â””â”€â”€ fixtures/                     # Test data and fixtures
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ scripts/                           # Executable scripts
â”‚   â”œâ”€â”€ api_server.py                     # API server launcher
â”‚   â”œâ”€â”€ train_model.py                    # Training script
â”‚   â”œâ”€â”€ chat_cli.py                       # CLI chat application
â”‚   â””â”€â”€ chat_web.py                       # Web chat application
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ config/                            # Configuration files
â”‚   â”œâ”€â”€ model_config.yaml                 # Model configuration
â”‚   â”œâ”€â”€ training_config.yaml              # Training parameters
â”‚   â””â”€â”€ logging_config.yaml               # Logging configuration
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ deployment/                        # Deployment configurations
â”‚   â”œâ”€â”€ docker/                           # Docker files
â”‚   â”‚   â”œâ”€â”€ Dockerfile                    # Main Dockerfile (GPU)
â”‚   â”‚   â”œâ”€â”€ Dockerfile.cpu                # CPU-only Dockerfile
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml            # Multi-service setup
â”‚   â”‚   â””â”€â”€ docker-demo.sh                # Deployment demo script
â”‚   â”œâ”€â”€ kubernetes/                       # Kubernetes manifests
â”‚   â”œâ”€â”€ nginx/                            # Nginx configuration
â”‚   â””â”€â”€ monitoring/                       # Monitoring configuration
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ docs/                              # Documentation
â”‚   â”œâ”€â”€ installation.md                   # Installation guide
â”‚   â”œâ”€â”€ usage.md                          # Usage examples
â”‚   â”œâ”€â”€ api_reference.md                  # API documentation
â”‚   â””â”€â”€ deployment_guide.md               # Deployment guide
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ examples/                          # Usage examples
â”‚   â”œâ”€â”€ basic_usage.py                    # Basic model usage
â”‚   â”œâ”€â”€ custom_training.py                # Custom training example
â”‚   â””â”€â”€ api_client.py                     # API client example
â”œâ”€â”€ 
â”œâ”€â”€ ğŸ“ data/                              # Data directory
â”‚   â”œâ”€â”€ raw/                              # Raw datasets
â”‚   â”œâ”€â”€ processed/                        # Processed datasets
â”‚   â””â”€â”€ samples/                          # Sample data
â”œâ”€â”€ 
â””â”€â”€ ğŸ“ models/                            # Model files
    â”œâ”€â”€ base/                             # Base model files
    â”œâ”€â”€ checkpoints/                      # Training checkpoints
    â””â”€â”€ exports/                          # Exported models
```

## ğŸ’» Usage Examples

### **Basic Model Usage**

```python
from thai_model import ThaiModel, ModelConfig

# Initialize model
config = ModelConfig.from_yaml("config/model_config.yaml")
model = ThaiModel(config)

# Generate text
response = model.generate_text("à¸ªà¸§à¸±à¸ªà¸”à¸µ à¸—à¹ˆà¸²à¸™à¹€à¸›à¹‡à¸™à¸­à¸¢à¹ˆà¸²à¸‡à¹„à¸£à¸šà¹‰à¸²à¸‡")
print(response)

# Chat completion
messages = [
    {"role": "user", "content": "à¸­à¸˜à¸´à¸šà¸²à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œ"}
]
response = model.chat_completion(messages)
print(response)

# Text summarization
text = "à¸‚à¹ˆà¸²à¸§à¸¢à¸²à¸§ à¹† à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ªà¸£à¸¸à¸›..."
summary = model.summarize_text(text, max_length=100)
print(summary)
```

### **API Client Usage**

```python
import requests

# Chat completion
response = requests.post(
    "http://localhost:8001/v1/chat/completions",
    json={
        "model": "thai-model",
        "messages": [
            {"role": "user", "content": "à¸ªà¸§à¸±à¸ªà¸”à¸µ"}
        ],
        "max_tokens": 100
    }
)
print(response.json())

# Text summarization
response = requests.post(
    "http://localhost:8001/v1/summarize",
    json={
        "text": "à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ªà¸£à¸¸à¸›...",
        "max_tokens": 50
    }
)
print(response.json())
```

### **Training a Custom Model**

```python
from thai_model.training import ThaiModelTrainer
from thai_model.core.config import TrainingConfig

# Load training configuration
config = TrainingConfig.from_yaml("config/training_config.yaml")

# Initialize trainer
trainer = ThaiModelTrainer(config)

# Train model
trainer.train()

# Evaluate model
results = trainer.evaluate()
print(results)
```

## âš™ï¸ Configuration

### **Model Configuration**

Edit `config/model_config.yaml`:

```yaml
model:
  model_name: "Qwen/Qwen2.5-1.5B-Instruct"
  adapter_path: "./models/checkpoints/qwen_thai_lora"
  device: "auto"
  torch_dtype: "float16"
  max_new_tokens: 512
  temperature: 0.7

api:
  host: "0.0.0.0"
  port: 8001
  workers: 1
```

### **Training Configuration**

Edit `config/training_config.yaml`:

```yaml
model:
  base_model: "Qwen/Qwen2.5-1.5B-Instruct"
  output_dir: "./models/checkpoints"

lora:
  r: 16
  alpha: 32
  dropout: 0.05

training:
  num_train_epochs: 3
  learning_rate: 2.0e-4
  per_device_train_batch_size: 1
```

## ğŸ“š API Documentation

### **Endpoints**

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/` | API information |
| GET | `/health` | Health check |
| GET | `/v1/models` | List available models |
| POST | `/v1/chat/completions` | OpenAI-compatible chat |
| POST | `/v1/summarize` | Thai text summarization |
| POST | `/v1/generate` | General text generation |

### **Chat Completions**

```bash
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "thai-model",
    "messages": [
      {"role": "user", "content": "à¸ªà¸§à¸±à¸ªà¸”à¸µ"}
    ],
    "max_tokens": 100,
    "temperature": 0.7,
    "stream": false
  }'
```

### **Text Summarization**

```bash
curl -X POST http://localhost:8001/v1/summarize \
  -H "Content-Type: application/json" \
  -d '{
    "text": "à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§ à¹† à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¸ªà¸£à¸¸à¸›...",
    "max_tokens": 100,
    "temperature": 0.7
  }'
```

For complete API documentation, visit `/docs` when the server is running.

## ğŸ³ Deployment

### **Docker Deployment**

```bash
# Build image
cd deployment/docker
docker build -f Dockerfile.cpu -t thai-model-api .

# Run container
docker run -d -p 8001:8001 \
  -v ./models:/app/models:ro \
  -v ./config:/app/config:ro \
  thai-model-api

# Or use docker-compose
docker-compose up -d
```

### **Kubernetes Deployment**

```bash
# Apply Kubernetes manifests
kubectl apply -f deployment/kubernetes/
```

### **Production Setup**

For production deployment with monitoring:

```bash
# Start full production stack
cd deployment/docker
docker-compose -f docker-compose.prod.yml up -d
```

This includes:
- API server with load balancing
- Nginx reverse proxy
- Prometheus metrics collection
- Grafana dashboards
- Redis caching

## ğŸ› ï¸ Development

### **Setting Up Development Environment**

```bash
# Clone repository
git clone https://github.com/username/thai-language-model.git
cd thai-language-model

# Install in development mode
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install
```

### **Running Tests**

```bash
# Run all tests
pytest

# Run specific test categories
pytest -m "not slow"  # Skip slow tests
pytest thai_model/tests/test_api.py  # API tests only

# Run with coverage
pytest --cov=thai_model --cov-report=html
```

### **Code Formatting**

```bash
# Format code
black thai_model/ scripts/
isort thai_model/ scripts/

# Type checking
mypy thai_model/
```

### **Building Documentation**

```bash
# Install docs dependencies
pip install -e ".[docs]"

# Build documentation
cd docs/
make html
```

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

### **Development Workflow**

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and ensure they pass
5. Submit a pull request

### **Reporting Issues**

Please use the [GitHub Issues](https://github.com/username/thai-language-model/issues) page to report bugs or request features.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Qwen Team** for the base model
- **Hugging Face** for the transformers library
- **Thai NLP Community** for datasets and resources
- **Contributors** who have helped improve this project

## ğŸ“ Support

- **Documentation**: [docs/](docs/)
- **Issues**: [GitHub Issues](https://github.com/username/thai-language-model/issues)
- **Discussions**: [GitHub Discussions](https://github.com/username/thai-language-model/discussions)

---

**Built with â¤ï¸ for the Thai language AI community**