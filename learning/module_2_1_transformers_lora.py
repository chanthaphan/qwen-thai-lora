#!/usr/bin/env python3
"""
Module 2.1: Understanding Transformers & LoRA
============================================

Interactive learning script to understand transformer architecture and LoRA fine-tuning.
"""

import sys
from pathlib import Path

def print_header(title):
    """Print a formatted header."""
    print(f"\n{'='*60}")
    print(f"ğŸ“ {title}")
    print(f"{'='*60}\n")

def print_step(step_num, description):
    """Print a formatted step."""
    print(f"ğŸ“š Step {step_num}: {description}")
    print("-" * 40)

def explain_transformer_architecture():
    """Explain transformer architecture concepts."""
    print("""
ğŸ§  Transformer Architecture Overview:

1. ğŸ”¤ Input Embedding Layer
   â€¢ Converts tokens to dense vectors
   â€¢ Position encoding adds sequence information

2. ğŸ¯ Multi-Head Attention Mechanism
   â€¢ Query (Q), Key (K), Value (V) matrices
   â€¢ Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
   â€¢ Multiple attention heads capture different relationships

3. ğŸ”„ Feed-Forward Networks
   â€¢ Two linear transformations with ReLU activation
   â€¢ FFN(x) = max(0, xW1 + b1)W2 + b2

4. ğŸ—ï¸ Layer Normalization & Residual Connections
   â€¢ LayerNorm(x + Sublayer(x))
   â€¢ Helps with gradient flow and training stability

5. ğŸ“¤ Output Layer
   â€¢ Linear layer projects to vocabulary size
   â€¢ Softmax for probability distribution over tokens
""")

def explain_attention_mechanism():
    """Explain attention mechanism in detail."""
    print("""
ğŸ¯ Self-Attention Mechanism Deep Dive:

ğŸ’¡ The Intuition:
   When processing "The cat sat on the mat", for the word "sat":
   â€¢ Query: "What am I looking for?"
   â€¢ Key: "What can be attended to?"
   â€¢ Value: "What information to extract?"

ğŸ”¢ The Math:
   1. Create Q, K, V matrices from input embeddings
   2. Compute attention scores: scores = Q Ã— K^T
   3. Scale by âˆšd_k to prevent vanishing gradients
   4. Apply softmax to get attention weights
   5. Multiply by V to get weighted representations

ğŸŒŸ Why It Works:
   â€¢ Captures long-range dependencies
   â€¢ Parallel computation (unlike RNNs)
   â€¢ Different heads focus on different relationships
   â€¢ Position-independent but position-aware
""")

def explain_lora():
    """Explain LoRA (Low-Rank Adaptation) technique."""
    print("""
ğŸš€ LoRA (Low-Rank Adaptation) Explained:

â“ The Problem:
   â€¢ Full fine-tuning requires updating ALL parameters
   â€¢ Qwen2.5-1.5B has ~1.5 billion parameters
   â€¢ Memory and compute intensive

ğŸ’¡ The LoRA Solution:
   Instead of updating W â†’ W + Î”W where Î”W is full rank,
   LoRA approximates: Î”W â‰ˆ A Ã— B
   
   Where:
   â€¢ A has shape (d, r) 
   â€¢ B has shape (r, d)
   â€¢ r << d (rank is much smaller than dimension)

ğŸ”¢ The Math:
   Original: h = Wx
   With LoRA: h = Wx + Î”Wx = Wx + BAx
   
   Parameters:
   â€¢ Original: d Ã— d parameters
   â€¢ LoRA: d Ã— r + r Ã— d = 2dr parameters
   â€¢ If d=4096, r=16: 16M vs 0.13M parameters! (99.2% reduction)

âš™ï¸ Key Parameters:
   â€¢ r (rank): Higher = more expressive but more parameters
   â€¢ Î± (alpha): Scaling factor, typically 2Ã—r
   â€¢ Target modules: Which layers to apply LoRA to
   â€¢ Dropout: Regularization for LoRA layers

ğŸ¯ Benefits:
   â€¢ 99%+ parameter reduction
   â€¢ Faster training and inference
   â€¢ Multiple adapters can be swapped
   â€¢ Original model stays frozen
""")

def demonstrate_tokenization():
    """Demonstrate Thai tokenization concepts."""
    print("""
ğŸ”¤ Thai Language Tokenization Challenges:

ğŸŒ Thai Language Characteristics:
   â€¢ No spaces between words: "à¸‰à¸±à¸™à¸£à¸±à¸à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢"
   â€¢ Complex script with vowels above/below consonants
   â€¢ Compound words and Sanskrit loanwords
   â€¢ Context-dependent word boundaries

ğŸ› ï¸ Tokenization Strategies:

1. ğŸ“ Character-level:
   "à¸ªà¸§à¸±à¸ªà¸”à¸µ" â†’ ['à¸ª', 'à¸§', 'à¸±', 'à¸ª', 'à¸”', 'à¸µ']
   + Handles any text
   - Long sequences, loses word meaning

2. ğŸ”¤ Subword (BPE/SentencePiece):
   "à¸ªà¸§à¸±à¸ªà¸”à¸µ" â†’ ['à¸ªà¸§à¸±à¸ª', 'à¸”à¸µ'] or ['à¸ª', 'à¸§à¸±à¸ªà¸”à¸µ']
   + Balance between coverage and meaning
   + Handles rare words

3. ğŸ“– Word-level:
   "à¸‰à¸±à¸™à¸£à¸±à¸à¹„à¸—à¸¢" â†’ ['à¸‰à¸±à¸™', 'à¸£à¸±à¸', 'à¹„à¸—à¸¢'] (requires word segmentation)
   + Preserves semantic meaning
   - Vocabulary explosion, OOV issues

ğŸ¯ Modern Approach (Qwen2.5):
   Uses SentencePiece with vocabulary of ~150K tokens
   Trained on multilingual data including Thai
""")

def main():
    print_header("Module 2.1: Understanding Transformers & LoRA")
    
    project_root = Path(__file__).parent.parent
    
    # Step 1: Transformer Architecture
    print_step(1, "Transformer Architecture Fundamentals")
    explain_transformer_architecture()
    
    input("\nğŸ” Press Enter to learn about attention mechanism...")
    
    # Step 2: Attention Mechanism
    print_step(2, "Self-Attention Mechanism")
    explain_attention_mechanism()
    
    input("\nğŸ” Press Enter to learn about LoRA...")
    
    # Step 3: LoRA Technique
    print_step(3, "LoRA (Low-Rank Adaptation)")
    explain_lora()
    
    input("\nğŸ” Press Enter to learn about tokenization...")
    
    # Step 4: Thai Tokenization
    print_step(4, "Thai Language Tokenization")
    demonstrate_tokenization()
    
    input("\nğŸ” Press Enter to explore the model code...")
    
    # Step 5: Exploring Model Code
    print_step(5, "Exploring Thai Model Implementation")
    
    try:
        sys.path.insert(0, str(project_root))
        
        # Show model structure
        model_file = project_root / "thai_model" / "core" / "model.py"
        if model_file.exists():
            print(f"ğŸ“„ Examining {model_file}:")
            
            with open(model_file, 'r') as f:
                lines = f.readlines()
            
            # Show class structure
            in_class = False
            indent_level = 0
            for i, line in enumerate(lines[:50]):  # First 50 lines
                if 'class ThaiModel' in line:
                    in_class = True
                    indent_level = len(line) - len(line.lstrip())
                    print(f"Line {i+1:2}: {line.rstrip()}")
                elif in_class:
                    if line.strip() and len(line) - len(line.lstrip()) <= indent_level:
                        break
                    if 'def ' in line or '__init__' in line:
                        print(f"Line {i+1:2}: {line.rstrip()}")
        
        # Try to show config structure
        config_file = project_root / "thai_model" / "core" / "config.py"
        if config_file.exists():
            print(f"\nğŸ“„ ModelConfig structure:")
            with open(config_file, 'r') as f:
                content = f.read()
            
            # Extract ModelConfig class
            if '@dataclass' in content and 'class ModelConfig' in content:
                lines = content.split('\n')
                in_modelconfig = False
                for line in lines:
                    if 'class ModelConfig' in line:
                        in_modelconfig = True
                        print(f"  {line.strip()}")
                    elif in_modelconfig and line.strip().startswith(('model_', 'max_', 'temperature', 'device')):
                        print(f"  {line.strip()}")
                    elif in_modelconfig and line.strip() and not line.startswith('    '):
                        break
    
    except Exception as e:
        print(f"âŒ Error exploring code: {e}")
        print("ğŸ’¡ The model files are available for manual inspection")
    
    input("\nğŸ” Press Enter to see practical examples...")
    
    # Step 6: Practical Examples
    print_step(6, "Practical Examples & Next Steps")
    
    print("""
ğŸ§ª Hands-on Experiments You Can Try:

1. ğŸ” Explore Model Architecture:
   ```python
   from transformers import AutoModel
   model = AutoModel.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
   print(model)  # See the full architecture
   ```

2. ğŸ”¤ Experiment with Tokenization:
   ```python
   from transformers import AutoTokenizer
   tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
   
   text = "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸œà¸¡à¸Šà¸·à¹ˆà¸­ AI"
   tokens = tokenizer.encode(text)
   print(f"Tokens: {tokens}")
   print(f"Decoded: {tokenizer.decode(tokens)}")
   ```

3. âš™ï¸ Examine LoRA Configuration:
   ```python
   from peft import LoraConfig
   
   lora_config = LoraConfig(
       r=16,                    # Rank
       lora_alpha=32,          # Alpha scaling
       target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],
       lora_dropout=0.1,       # Dropout
   )
   print(lora_config)
   ```

4. ğŸ“Š Check Model Parameters:
   ```python
   def count_parameters(model):
       return sum(p.numel() for p in model.parameters() if p.requires_grad)
   
   # Compare base model vs LoRA model parameter counts
   ```

ğŸ“š Study Materials for Deep Dive:
  â€¢ "Attention Is All You Need" paper (Vaswani et al.)
  â€¢ "LoRA: Low-Rank Adaptation" paper (Hu et al.)
  â€¢ Hugging Face Transformers documentation
  â€¢ Thai NLP resources and datasets

ğŸ¯ Key Takeaways:
  â€¢ Transformers use self-attention for sequence modeling
  â€¢ LoRA enables efficient fine-tuning with minimal parameters
  â€¢ Thai requires special tokenization considerations
  â€¢ The thai_model package wraps these concepts cleanly

ğŸš€ Ready for Module 2.2: Model Training & Fine-tuning!
""")

if __name__ == "__main__":
    main()