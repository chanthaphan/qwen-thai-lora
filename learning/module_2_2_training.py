#!/usr/bin/env python3
"""
Module 2.2: Model Training & Fine-tuning
=======================================

Interactive learning script to master model training pipelines and fine-tuning techniques.
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime

def print_header(title):
    """Print a formatted header."""
    print(f"\n{'='*60}")
    print(f"ğŸ“ {title}")
    print(f"{'='*60}\n")

def print_step(step_num, description):
    """Print a formatted step."""
    print(f"ğŸ“š Step {step_num}: {description}")
    print("-" * 40)

def explain_training_pipeline():
    """Explain the model training pipeline."""
    print("""
ğŸ”„ Model Training Pipeline Overview:

1. ğŸ“Š Data Preparation
   â€¢ Load and preprocess Thai text data
   â€¢ Tokenization and sequence formatting
   â€¢ Train/validation/test split
   â€¢ Data collation and batching

2. ğŸ—ï¸ Model Setup
   â€¢ Load base model (Qwen2.5-1.5B-Instruct)
   â€¢ Apply LoRA adapters to target modules
   â€¢ Configure gradient checkpointing for memory efficiency
   â€¢ Set up mixed precision training (fp16/bf16)

3. âš™ï¸ Training Configuration
   â€¢ Learning rate scheduling (linear warmup)
   â€¢ Optimization (AdamW with paging)
   â€¢ Gradient accumulation and clipping
   â€¢ Evaluation strategy and metrics

4. ğŸš€ Training Loop
   â€¢ Forward pass with teacher forcing
   â€¢ Loss computation (cross-entropy)
   â€¢ Backward pass with gradient accumulation
   â€¢ Parameter updates and logging

5. ğŸ“ˆ Evaluation & Monitoring
   â€¢ Perplexity and BLEU score tracking
   â€¢ Loss curves and learning rate plots
   â€¢ Model checkpointing and early stopping
   â€¢ Validation on held-out data
""")

def explain_lora_training():
    """Explain LoRA training specifics."""
    print("""
ğŸ¯ LoRA Training Deep Dive:

ğŸ’¡ Key Concepts:
   â€¢ Only LoRA parameters are trainable (~0.1% of total)
   â€¢ Base model remains frozen throughout training
   â€¢ Multiple LoRA adapters can be trained for different tasks
   â€¢ Adapters can be merged back into base model

âš™ï¸ Critical Parameters:
   
   ğŸ“ Rank (r): 4, 8, 16, 32
      â€¢ Higher rank = more parameters but better expressiveness
      â€¢ Thai language typically works well with r=16
   
   ğŸ”¢ Alpha (Î±): Usually 2Ã—r (e.g., Î±=32 for r=16)
      â€¢ Controls the scaling of LoRA updates
      â€¢ Higher alpha = stronger adaptation
   
   ğŸ¯ Target Modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
      â€¢ Which attention layers to apply LoRA to
      â€¢ Can also include MLP layers: ["gate_proj", "up_proj", "down_proj"]
   
   ğŸ’§ Dropout: 0.05 - 0.1
      â€¢ Regularization for LoRA layers only
      â€¢ Prevents overfitting to training data

ğŸ”„ Training Process:
   1. Base model forward pass (frozen parameters)
   2. LoRA forward pass: h = h + BA Ã— input
   3. Loss computation on combined output
   4. Gradients only flow through LoRA parameters
   5. Update only A and B matrices

ğŸ“Š Memory Benefits:
   â€¢ Full fine-tuning: ~6GB VRAM for 1.5B model
   â€¢ LoRA training: ~2-3GB VRAM for same model
   â€¢ Enables training on consumer GPUs
""")

def demonstrate_training_config():
    """Demonstrate training configuration."""
    print("""
ğŸ› ï¸ Training Configuration Breakdown:

ğŸ“‹ Essential Training Arguments:
""")
    
    # Show actual training config
    project_root = Path(__file__).parent.parent
    config_path = project_root / "config" / "training_config.yaml"
    
    if config_path.exists():
        try:
            import yaml
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            print("ğŸ”§ Current Training Configuration:")
            
            # Show key sections with explanations
            sections = {
                'lora': 'ğŸ¯ LoRA Configuration',
                'training': 'ğŸ“ˆ Training Parameters', 
                'dataset': 'ğŸ“Š Dataset Settings',
                'evaluation': 'ğŸ§ª Evaluation Strategy',
                'logging': 'ğŸ“ Logging & Checkpointing'
            }
            
            for section, title in sections.items():
                if section in config:
                    print(f"\n{title}:")
                    for key, value in config[section].items():
                        print(f"  â€¢ {key}: {value}")
                        
                        # Add explanations for key parameters
                        explanations = {
                            'r': 'LoRA rank - higher = more parameters',
                            'alpha': 'LoRA scaling factor - typically 2Ã—rank',
                            'learning_rate': 'Step size for parameter updates',
                            'per_device_train_batch_size': 'Samples per GPU per step',
                            'gradient_accumulation_steps': 'Steps before parameter update',
                            'max_seq_length': 'Maximum input sequence length',
                            'eval_steps': 'Frequency of evaluation runs'
                        }
                        
                        if key in explanations:
                            print(f"    â””â”€ {explanations[key]}")
            
        except Exception as e:
            print(f"âŒ Error reading config: {e}")
    else:
        print("âŒ Training config file not found")

def show_dataset_preparation():
    """Show dataset preparation concepts."""
    print("""
ğŸ“Š Thai Dataset Preparation:

ğŸŒ Thai Language Challenges:
   â€¢ No word boundaries: "à¸‰à¸±à¸™à¹„à¸›à¹‚à¸£à¸‡à¹€à¸£à¸µà¸¢à¸™" (I go to school)
   â€¢ Complex script with tone marks and vowel positions
   â€¢ Mixed script: Thai + English + numbers
   â€¢ Formal vs informal language styles

ğŸ“ Data Preprocessing Pipeline:

1. ğŸ§¹ Text Cleaning:
   ```python
   def clean_thai_text(text):
       # Remove excessive whitespace
       text = re.sub(r'\s+', ' ', text)
       # Normalize Thai characters
       text = unicodedata.normalize('NFKC', text)
       # Remove special characters but keep Thai punctuation
       return text.strip()
   ```

2. ğŸ”¤ Tokenization Strategy:
   ```python
   # Using Qwen tokenizer for Thai
   tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
   
   # Example Thai text
   text = "à¸ªà¸§à¸±à¸ªà¸”à¸µà¸„à¸£à¸±à¸š à¸œà¸¡à¹€à¸›à¹‡à¸™à¹‚à¸¡à¹€à¸”à¸¥à¸ à¸²à¸©à¸²à¹„à¸—à¸¢"
   tokens = tokenizer.encode(text)
   
   # Check token efficiency
   compression_ratio = len(text) / len(tokens)
   print(f"Compression: {compression_ratio:.2f} chars per token")
   ```

3. ğŸ“ Sequence Formatting:
   ```python
   def format_chat_template(conversation):
       formatted = tokenizer.apply_chat_template(
           conversation,
           tokenize=False,
           add_generation_prompt=False
       )
       return formatted
   
   # Example conversation
   conversation = [
       {"role": "user", "content": "à¸­à¸˜à¸´à¸šà¸²à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡ AI"},
       {"role": "assistant", "content": "AI à¸«à¸£à¸·à¸­à¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œ..."}
   ]
   ```

ğŸ“ˆ Data Augmentation Techniques:
   â€¢ Back-translation (Thai â†’ English â†’ Thai)
   â€¢ Paraphrasing with existing models
   â€¢ Synthetic data generation
   â€¢ Code-switching examples (Thai-English mix)
""")

def explain_evaluation_metrics():
    """Explain model evaluation metrics."""
    print("""
ğŸ“ Model Evaluation Metrics:

ğŸ¯ Language Generation Metrics:

1. ğŸ“‰ Perplexity (PPL):
   â€¢ Measures how "surprised" the model is by test data
   â€¢ Lower is better: PPL = exp(cross_entropy_loss)
   â€¢ Good Thai models: PPL < 10-15

2. ğŸ¨ BLEU Score (0-100):
   â€¢ Compares generated text to reference translations
   â€¢ Measures n-gram overlap precision
   â€¢ Good for translation tasks: BLEU > 20-30

3. ğŸ” ROUGE Scores:
   â€¢ ROUGE-L: Longest common subsequence
   â€¢ Good for summarization: ROUGE-L > 0.3-0.4

4. ğŸ§  Semantic Similarity:
   â€¢ Embedding-based similarity (cosine distance)
   â€¢ Captures meaning beyond surface form
   â€¢ Use multilingual embeddings for Thai

ğŸ’¡ Thai-Specific Evaluation:

1. ğŸ“ Word Segmentation Quality:
   ```python
   # Test if model maintains proper Thai word boundaries
   input_text = "à¸‰à¸±à¸™à¹„à¸›à¹‚à¸£à¸‡à¹€à¸£à¸µà¸¢à¸™"
   output_text = model.generate(input_text)
   
   # Check if output preserves linguistic structure
   ```

2. ğŸ­ Politeness Level Consistency:
   â€¢ Thai has multiple politeness levels (à¸à¸£à¸¸à¸“à¸², à¸‚à¸­, à¸„à¸£à¸±à¸š/à¸„à¹ˆà¸°)
   â€¢ Model should maintain consistent register
   â€¢ Evaluate across formal/informal contexts

3. ğŸŒ Code-Switching Handling:
   â€¢ Thai-English mixed sentences
   â€¢ Technical terms in English within Thai context
   â€¢ Proper script switching

ğŸ“Š Automated Evaluation Setup:
   ```python
   def evaluate_thai_model(model, eval_dataset):
       results = {
           'perplexity': [],
           'bleu_scores': [],
           'rouge_scores': [],
           'generation_examples': []
       }
       
       for batch in eval_dataset:
           # Compute perplexity
           with torch.no_grad():
               outputs = model(**batch)
               ppl = torch.exp(outputs.loss)
               results['perplexity'].append(ppl.item())
           
           # Generate and evaluate BLEU
           generated = model.generate(batch['input_ids'])
           bleu = compute_bleu(generated, batch['labels'])
           results['bleu_scores'].append(bleu)
       
       return results
   ```
""")

def main():
    print_header("Module 2.2: Model Training & Fine-tuning")
    
    project_root = Path(__file__).parent.parent
    
    # Step 1: Training Pipeline Overview
    print_step(1, "Understanding the Training Pipeline")
    explain_training_pipeline()
    
    input("\nğŸ” Press Enter to learn about LoRA training...")
    
    # Step 2: LoRA Training Deep Dive
    print_step(2, "LoRA Training Deep Dive")
    explain_lora_training()
    
    input("\nğŸ” Press Enter to explore training configuration...")
    
    # Step 3: Training Configuration
    print_step(3, "Training Configuration Analysis")
    demonstrate_training_config()
    
    input("\nğŸ” Press Enter to learn about dataset preparation...")
    
    # Step 4: Dataset Preparation
    print_step(4, "Thai Dataset Preparation")
    show_dataset_preparation()
    
    input("\nğŸ” Press Enter to learn about evaluation...")
    
    # Step 5: Evaluation Metrics
    print_step(5, "Model Evaluation Metrics")
    explain_evaluation_metrics()
    
    input("\nğŸ” Press Enter to see practical exercises...")
    
    # Step 6: Practical Exercises
    print_step(6, "Practical Training Exercises")
    
    print("""
ğŸ§ª Hands-on Training Experiments:

1. ğŸ“Š Analyze Your Training Data:
   ```python
   # Explore the training configuration
   import yaml
   with open('config/training_config.yaml', 'r') as f:
       config = yaml.safe_load(f)
   
   print("LoRA Config:", config['lora'])
   print("Batch Size:", config['training']['per_device_train_batch_size'])
   print("Learning Rate:", config['training']['learning_rate'])
   ```

2. ğŸ”§ Calculate Training Parameters:
   ```python
   # Estimate training time and memory
   def estimate_training_resources(config):
       batch_size = config['training']['per_device_train_batch_size']
       grad_accum = config['training']['gradient_accumulation_steps']
       effective_batch = batch_size * grad_accum
       
       print(f"Effective batch size: {effective_batch}")
       print(f"LoRA rank: {config['lora']['r']}")
       
       # Estimate parameters
       base_params = 1.5e9  # 1.5B model
       lora_params = estimate_lora_params(config['lora'])
       print(f"LoRA parameters: {lora_params:,.0f} ({lora_params/base_params*100:.3f}% of base)")
   ```

3. ğŸ¯ Experiment with LoRA Settings:
   ```python
   # Try different LoRA configurations
   lora_experiments = [
       {'r': 8, 'alpha': 16},   # Small, fast
       {'r': 16, 'alpha': 32},  # Balanced (current)
       {'r': 32, 'alpha': 64},  # Large, expressive
   ]
   
   for config in lora_experiments:
       params = 2 * config['r'] * 4096  # Approximate for attention layers
       print(f"Rank {config['r']}: ~{params:,.0f} parameters")
   ```

4. ğŸ“ˆ Monitor Training Progress:
   ```bash
   # If you have a trained model, examine the logs
   ls -la models/qwen_thai_lora/
   
   # Look for training logs and checkpoints
   find models/ -name "*.log" -o -name "trainer_state.json"
   ```

5. ğŸ§® Evaluate Model Performance:
   ```python
   # Load and test a checkpoint
   from transformers import AutoTokenizer
   from peft import PeftModel
   
   # Load base model and adapter
   base_model = "Qwen/Qwen2.5-1.5B-Instruct"
   adapter_path = "models/qwen_thai_lora"
   
   tokenizer = AutoTokenizer.from_pretrained(base_model)
   
   # Test Thai generation
   prompt = "à¸­à¸˜à¸´à¸šà¸²à¸¢à¹€à¸£à¸·à¹ˆà¸­à¸‡à¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œ"
   inputs = tokenizer(prompt, return_tensors="pt")
   
   # Compare with base model vs fine-tuned model
   ```

ğŸ“š Advanced Training Topics to Explore:

ğŸ”¬ Hyperparameter Tuning:
   â€¢ Learning rate scheduling strategies
   â€¢ Batch size vs convergence speed trade-offs
   â€¢ Gradient accumulation for memory constraints
   â€¢ Early stopping criteria

ğŸ“Š Data Quality & Augmentation:
   â€¢ Creating high-quality Thai instruction datasets
   â€¢ Balancing formal vs informal language
   â€¢ Handling domain-specific terminology
   â€¢ Quality filtering techniques

ğŸ¯ Specialized Fine-tuning:
   â€¢ Task-specific adapters (summarization, QA, chat)
   â€¢ Multi-task learning with shared base
   â€¢ Instruction tuning vs chat fine-tuning
   â€¢ Constitutional AI principles

âš¡ Optimization Techniques:
   â€¢ Gradient checkpointing for memory efficiency
   â€¢ Mixed precision training (fp16/bf16)
   â€¢ DeepSpeed integration for large models
   â€¢ Model parallelism strategies

ğŸ¯ Key Takeaways:
   â€¢ LoRA enables efficient fine-tuning with minimal resources
   â€¢ Thai language requires special tokenization considerations
   â€¢ Proper evaluation metrics are crucial for Thai models
   â€¢ Training configuration significantly impacts results

ğŸš€ Ready for Module 3.1: FastAPI Fundamentals!
""")

if __name__ == "__main__":
    main()