import os
import sys
import json
import requests

# 1) ‡∏≠‡πà‡∏≤‡∏ô‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á Ollama ‡∏à‡∏≤‡∏Å env (‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤)
OLLAMA_HOST = os.environ.get("OLLAMA_HOST", "127.0.0.1:11434")

# 2) ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
MODEL = os.environ.get("OLLAMA_MODEL", "llama3.1:8b")

# 3) helper: ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å /api/generate ‡πÅ‡∏ö‡∏ö non-stream (‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß)
def generate(prompt: str, system: str | None = None, temperature: float = 0.7, max_tokens: int = 512) -> str:
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Ollama /api/generate
    - prompt: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    - system: system prompt (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)
    - temperature: ‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∏‡πà‡∏° (0-1 ‡∏ï‡πà‡∏≥=‡∏Ñ‡∏á‡∏ó‡∏µ‡πà ‡∏™‡∏π‡∏á=‡∏Ñ‡∏£‡∏µ‡πÄ‡∏≠‡∏ó‡∏µ‡∏ü)
    - max_tokens: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô token ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    """
    url = f"http://{OLLAMA_HOST}/api/generate"
    payload = {
        "model": MODEL,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": temperature, "num_predict": max_tokens},
    }
    # ‡πÉ‡∏™‡πà system prompt ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    if system:
        payload["system"] = system

    # ‡∏¢‡∏¥‡∏á HTTP POST ‡πÑ‡∏õ‡∏ó‡∏µ‡πà Ollama
    res = requests.post(url, json=payload, timeout=600)
    res.raise_for_status()  # ‡∏ñ‡πâ‡∏≤ HTTP != 200 ‡∏à‡∏∞ throw error
    data = res.json()       # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict
    return data.get("response", "").strip()

# 4) ‡πÇ‡∏´‡∏°‡∏î chat loop ‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡πÉ‡∏ô terminal
def chat_loop():
    """
    loop ‡∏Ñ‡∏∏‡∏¢‡∏ó‡∏µ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î:
    - 'exit' ‡∏´‡∏£‡∏∑‡∏≠ Ctrl+C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏≠‡∏Å
    """
    print(f"Connected to Ollama at http://{OLLAMA_HOST} with model '{MODEL}'")
    print("Type your message. Type 'exit' to quit.\n")

    # system prompt ‡∏ï‡∏±‡πâ‡∏á‡∏Å‡∏ï‡∏¥‡∏Å‡∏≤‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏ó (‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å)
    system_prompt = (
        "You are a helpful bilingual assistant. "
        "Reply in Thai by default but include concise English when useful."
    )

    while True:
        try:
            user_msg = input("‡∏Ñ‡∏∏‡∏ì: ").strip()
            if user_msg.lower() in {"exit", "quit"}:
                print("‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ üëã")
                break

            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å generate() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≠‡∏ö
            reply = generate(user_msg, system=system_prompt, temperature=0.7, max_tokens=512)
            print("\n‡∏ö‡∏≠‡∏ó:", reply, "\n")

        except KeyboardInterrupt:
            print("\n‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏° üëã")
            break
        except requests.RequestException as e:
            print(f"\n[HTTP ERROR] {e}\n")
        except Exception as e:
            print(f"\n[ERROR] {type(e).__name__}: {e}\n")

if __name__ == "__main__":
    # 5) ‡∏ó‡∏≤‡∏á‡∏•‡∏±‡∏î: ‡∏ñ‡πâ‡∏≤‡∏û‡∏¥‡∏°‡∏û‡πå argument ‡∏ï‡πà‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå ‡∏à‡∏∞‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏ö
    #    ‡πÄ‡∏ä‡πà‡∏ô: python chat_ollama.py "‡∏™‡∏£‡∏∏‡∏õ LLM ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£ 3 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î"
    if len(sys.argv) > 1:
        prompt = " ".join(sys.argv[1:])
        ans = generate(prompt, system="‡∏ï‡∏≠‡∏ö‡∏™‡∏±‡πâ‡∏ô ‡∏ä‡∏±‡∏î ‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏†‡∏≤‡∏û", temperature=0.7, max_tokens=256)
        print(ans)
    else:
        chat_loop()
