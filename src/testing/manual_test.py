#!/usr/bin/env python3
"""
Interactive Test Script
Quick manual testing of the Thai model
"""

import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def load_model():
    """Load the Thai model"""
    print("ğŸ”§ Loading Thai model...")
    
    base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    lora_path = "./models/qwen_thai_lora"
    
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    model = PeftModel.from_pretrained(base_model, lora_path)
    model.eval()
    
    print("âœ… Model loaded successfully!")
    return model, tokenizer

def test_summarization(model, tokenizer, text):
    """Test summarization on given text"""
    prompt = f"à¸ªà¸£à¸¸à¸›à¸‚à¹ˆà¸²à¸§à¸•à¹ˆà¸­à¹„à¸›à¸™à¸µà¹‰:\n\n{text}\n\nà¸ªà¸£à¸¸à¸›:"
    
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=450)
    if torch.cuda.is_available():
        inputs = {k: v.cuda() for k, v in inputs.items()}
    
    print("ğŸ”„ Generating summary...")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            no_repeat_ngram_size=3
        )
    
    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Extract summary
    if "à¸ªà¸£à¸¸à¸›:" in generated:
        summary = generated.split("à¸ªà¸£à¸¸à¸›:")[-1].strip()
    else:
        summary = generated.strip()
    
    return summary

def interactive_test():
    """Interactive testing mode"""
    model, tokenizer = load_model()
    
    print("\nğŸ§ª Interactive Thai Model Testing")
    print("=" * 50)
    print("Type 'quit' to exit, 'help' for sample texts")
    
    sample_texts = [
        "à¸™à¸±à¸à¸§à¸´à¸—à¸¢à¸²à¸¨à¸²à¸ªà¸•à¸£à¹Œà¹„à¸”à¹‰à¸à¸±à¸’à¸™à¸²à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¹ƒà¸«à¸¡à¹ˆà¸—à¸µà¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸•à¸£à¸§à¸ˆà¸ˆà¸±à¸šà¹‚à¸£à¸„à¸¡à¸°à¹€à¸£à¹‡à¸‡à¹„à¸”à¹‰à¹à¸¡à¹ˆà¸™à¸¢à¸³à¸‚à¸¶à¹‰à¸™ à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰à¸›à¸±à¸à¸à¸²à¸›à¸£à¸°à¸”à¸´à¸©à¸à¹Œà¹ƒà¸™à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸ à¸²à¸à¸–à¹ˆà¸²à¸¢à¸—à¸²à¸‡à¸à¸²à¸£à¹à¸à¸—à¸¢à¹Œ",
        "à¸£à¸±à¸à¸šà¸²à¸¥à¸›à¸£à¸°à¸à¸²à¸¨à¸™à¹‚à¸¢à¸šà¸²à¸¢à¸ªà¹ˆà¸‡à¹€à¸ªà¸£à¸´à¸¡à¸à¸¥à¸±à¸‡à¸‡à¸²à¸™à¸ªà¸°à¸­à¸²à¸” à¹€à¸à¸·à¹ˆà¸­à¸¥à¸”à¸à¸²à¸£à¸›à¸¥à¹ˆà¸­à¸¢à¸à¹Šà¸²à¸‹à¹€à¸£à¸·à¸­à¸™à¸à¸£à¸°à¸ˆà¸à¹à¸¥à¸°à¸ªà¸™à¸±à¸šà¸ªà¸™à¸¸à¸™à¹€à¸—à¸„à¹‚à¸™à¹‚à¸¥à¸¢à¸µà¸—à¸µà¹ˆà¹€à¸›à¹‡à¸™à¸¡à¸´à¸•à¸£à¸à¸±à¸šà¸ªà¸´à¹ˆà¸‡à¹à¸§à¸”à¸¥à¹‰à¸­à¸¡",
        "à¸à¸²à¸£à¸¨à¸¶à¸à¸©à¸²à¹ƒà¸«à¸¡à¹ˆà¸à¸šà¸§à¹ˆà¸²à¸à¸²à¸£à¸­à¸­à¸à¸à¸³à¸¥à¸±à¸‡à¸à¸²à¸¢à¸ªà¸¡à¹ˆà¸³à¹€à¸ªà¸¡à¸­à¸Šà¹ˆà¸§à¸¢à¹€à¸à¸´à¹ˆà¸¡à¸„à¸§à¸²à¸¡à¸ªà¸¸à¸‚à¹à¸¥à¸°à¸¥à¸”à¸„à¸§à¸²à¸¡à¹€à¸„à¸£à¸µà¸¢à¸” à¸£à¸§à¸¡à¸—à¸±à¹‰à¸‡à¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡à¸„à¸¸à¸“à¸ à¸²à¸à¸à¸²à¸£à¸™à¸­à¸™à¸«à¸¥à¸±à¸š"
    ]
    
    while True:
        print(f"\nğŸ“ Enter Thai text to summarize:")
        user_input = input("> ").strip()
        
        if user_input.lower() == 'quit':
            print("ğŸ‘‹ Goodbye!")
            break
        elif user_input.lower() == 'help':
            print("\nğŸ“‹ Sample texts you can try:")
            for i, sample in enumerate(sample_texts, 1):
                print(f"{i}. {sample}")
            continue
        elif user_input.isdigit() and 1 <= int(user_input) <= len(sample_texts):
            user_input = sample_texts[int(user_input) - 1]
            print(f"ğŸ“„ Using sample text: {user_input}")
        elif not user_input:
            print("âš ï¸  Please enter some text")
            continue
        
        try:
            summary = test_summarization(model, tokenizer, user_input)
            print(f"\nâœ¨ Summary: {summary}")
            print(f"ğŸ“Š Original: {len(user_input)} chars â†’ Summary: {len(summary)} chars")
            
        except Exception as e:
            print(f"âŒ Error: {e}")

def main():
    """Main function"""
    if len(sys.argv) > 1:
        # Command line mode
        model, tokenizer = load_model()
        text = " ".join(sys.argv[1:])
        print(f"\nğŸ“„ Input text: {text}")
        
        try:
            summary = test_summarization(model, tokenizer, text)
            print(f"\nâœ¨ Summary: {summary}")
        except Exception as e:
            print(f"âŒ Error: {e}")
    else:
        # Interactive mode
        interactive_test()

if __name__ == "__main__":
    main()