#!/usr/bin/env python3
"""
Model Evaluation Script
This script evaluates the Thai model performance on various tasks
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import json
import time
from datetime import datetime

class ThaiModelEvaluator:
    def __init__(self, base_model_name="Qwen/Qwen2.5-1.5B-Instruct", lora_path="./models/qwen_thai_lora"):
        """Initialize the evaluator"""
        print("üîß Initializing Thai Model Evaluator...")
        
        self.base_model_name = base_model_name
        self.lora_path = lora_path
        
        # Load model
        print("üìÅ Loading tokenizer and model...")
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        self.model = PeftModel.from_pretrained(base_model, lora_path)
        self.model.eval()
        
        print("‚úÖ Model loaded successfully!")
    
    def generate_summary(self, text, max_tokens=100):
        """Generate a summary for given text"""
        prompt = f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:\n\n{text}\n\n‡∏™‡∏£‡∏∏‡∏õ:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=450)
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3
            )
        
        generation_time = time.time() - start_time
        generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract summary
        if "‡∏™‡∏£‡∏∏‡∏õ:" in generated:
            summary = generated.split("‡∏™‡∏£‡∏∏‡∏õ:")[-1].strip()
        else:
            summary = generated.strip()
        
        return summary, generation_time
    
    def evaluate_test_cases(self):
        """Run evaluation on predefined test cases"""
        test_cases = [
            {
                "id": 1,
                "category": "Technology",
                "text": "‡∏ö‡∏£‡∏¥‡∏©‡∏±‡∏ó‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥‡πÑ‡∏î‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ Deep Learning ‡∏Ç‡∏±‡πâ‡∏ô‡∏™‡∏π‡∏á ‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏ó‡∏≤‡∏á‡∏ß‡∏±‡∏í‡∏ô‡∏ò‡∏£‡∏£‡∏°‡πÑ‡∏î‡πâ‡∏î‡∏µ ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á‡∏ñ‡∏∂‡∏á 98% ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡πâ‡∏≤‡∏ß‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢",
                "expected_topics": ["‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ", "‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå", "‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤"]
            },
            {
                "id": 2,
                "category": "Health",
                "text": "‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ì‡∏∞‡πÅ‡∏û‡∏ó‡∏¢‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏ú‡∏•‡πÑ‡∏°‡πâ‡πÅ‡∏•‡∏∞‡∏ú‡∏±‡∏Å‡∏™‡∏µ‡πÄ‡∏Ç‡πâ‡∏°‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à‡πÑ‡∏î‡πâ‡∏ñ‡∏∂‡∏á 40% ‡πÇ‡∏î‡∏¢‡∏™‡∏≤‡∏£‡∏ï‡πâ‡∏≤‡∏ô‡∏≠‡∏ô‡∏∏‡∏°‡∏π‡∏•‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡πÉ‡∏ô‡∏ú‡∏±‡∏Å‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡πÑ‡∏°‡πâ‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏Å‡∏õ‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏≠‡∏î‡πÄ‡∏•‡∏∑‡∏≠‡∏î‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏Å‡πÄ‡∏™‡∏ö ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏ú‡∏•‡πÑ‡∏°‡πâ‡∏ú‡∏±‡∏Å‡∏™‡∏µ‡πÄ‡∏Ç‡πâ‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 5 ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô",
                "expected_topics": ["‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û", "‡πÇ‡∏£‡∏Ñ‡∏´‡∏±‡∏ß‡πÉ‡∏à", "‡∏ú‡∏•‡πÑ‡∏°‡πâ‡∏ú‡∏±‡∏Å"]
            },
            {
                "id": 3,
                "category": "Environment",
                "text": "‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏•‡∏π‡∏Å‡∏õ‡πà‡∏≤‡∏ä‡∏≤‡∏¢‡πÄ‡∏•‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡πÑ‡∏î‡πâ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÉ‡∏ô‡∏†‡∏≤‡∏Ñ‡πÉ‡∏ï‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ü‡∏∑‡πâ‡∏ô‡∏ü‡∏π‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏¥‡πÄ‡∏ß‡∏®‡∏ó‡∏≤‡∏á‡∏ó‡∏∞‡πÄ‡∏•‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏® ‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡∏Ñ‡∏≤‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏õ‡∏•‡∏π‡∏Å‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡∏Å‡∏ß‡πà‡∏≤ 100,000 ‡∏ï‡πâ‡∏ô ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà 500 ‡πÑ‡∏£‡πà ‡πÅ‡∏•‡∏∞‡∏à‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏π‡∏î‡∏ã‡∏±‡∏ö‡∏Ñ‡∏≤‡∏£‡πå‡∏ö‡∏≠‡∏ô‡πÑ‡∏î‡∏≠‡∏≠‡∏Å‡πÑ‡∏ã‡∏î‡πå‡πÑ‡∏î‡πâ‡∏õ‡∏µ‡∏•‡∏∞ 5,000 ‡∏ï‡∏±‡∏ô ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏Ç‡∏≠‡∏á‡∏™‡∏±‡∏ï‡∏ß‡πå‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏ô‡∏Å‡∏´‡∏≤‡∏¢‡∏≤‡∏Å",
                "expected_topics": ["‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°", "‡∏õ‡πà‡∏≤‡∏ä‡∏≤‡∏¢‡πÄ‡∏•‡∏ô", "‡∏™‡∏†‡∏≤‡∏û‡∏†‡∏π‡∏°‡∏¥‡∏≠‡∏≤‡∏Å‡∏≤‡∏®"]
            }
        ]
        
        results = []
        total_time = 0
        
        print("\nüß™ Running Model Evaluation...")
        print("=" * 60)
        
        for case in test_cases:
            print(f"\nüìã Test Case {case['id']} - {case['category']}")
            print("-" * 40)
            print(f"üìÑ Original: {case['text'][:100]}...")
            
            summary, gen_time = self.generate_summary(case['text'])
            total_time += gen_time
            
            print(f"‚ú® Summary: {summary}")
            print(f"‚è±Ô∏è  Generation time: {gen_time:.2f}s")
            
            # Simple evaluation metrics
            original_length = len(case['text'])
            summary_length = len(summary)
            compression_ratio = summary_length / original_length
            
            result = {
                "test_id": case['id'],
                "category": case['category'],
                "original_text": case['text'],
                "generated_summary": summary,
                "original_length": original_length,
                "summary_length": summary_length,
                "compression_ratio": compression_ratio,
                "generation_time": gen_time,
                "expected_topics": case['expected_topics']
            }
            
            results.append(result)
            
            print(f"üìä Length: {original_length} ‚Üí {summary_length} chars (compression: {compression_ratio:.2f})")
        
        # Summary statistics
        avg_time = total_time / len(test_cases)
        avg_compression = sum(r['compression_ratio'] for r in results) / len(results)
        
        print(f"\nüìà Evaluation Summary:")
        print(f"   ‚Ä¢ Total test cases: {len(test_cases)}")
        print(f"   ‚Ä¢ Average generation time: {avg_time:.2f}s")
        print(f"   ‚Ä¢ Average compression ratio: {avg_compression:.2f}")
        print(f"   ‚Ä¢ Total evaluation time: {total_time:.2f}s")
        
        return results
    
    def save_evaluation_report(self, results, filename=None):
        """Save evaluation results to JSON file"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"evaluation_report_{timestamp}.json"
        
        report = {
            "evaluation_date": datetime.now().isoformat(),
            "model_info": {
                "base_model": self.base_model_name,
                "lora_path": self.lora_path
            },
            "test_results": results,
            "summary_stats": {
                "total_cases": len(results),
                "avg_generation_time": sum(r['generation_time'] for r in results) / len(results),
                "avg_compression_ratio": sum(r['compression_ratio'] for r in results) / len(results)
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"üìÑ Evaluation report saved: {filename}")
        return filename

def main():
    """Main evaluation function"""
    try:
        evaluator = ThaiModelEvaluator()
        results = evaluator.evaluate_test_cases()
        report_file = evaluator.save_evaluation_report(results)
        
        print(f"\nüéâ Evaluation completed successfully!")
        print(f"üìã Report saved: {report_file}")
        
    except Exception as e:
        print(f"‚ùå Evaluation failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()