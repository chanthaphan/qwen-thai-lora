from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
import torch, os
import numpy as np
from rouge_score import rouge_scorer
from sklearn.metrics import accuracy_score
import json

# ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• Qwen2.5 ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ
model_name = "Qwen/Qwen2.5-1.5B-Instruct"   # ‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ
dataset_name = "pythainlp/thaisum"         # Thai summarization dataset (real one)

print(f"Loading model: {model_name}")

# ‡πÇ‡∏´‡∏•‡∏î tokenizer / model ‡∏û‡∏£‡πâ‡∏≠‡∏° error handling
try:
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    print("Tokenizer loaded successfully")
except Exception as e:
    print(f"Error loading tokenizer: {e}")
    # Fallback ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô
    model_name = "microsoft/DialoGPT-medium"
    print(f"Fallback to model: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° pad_token ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        load_in_8bit=True,            # ‡πÉ‡∏ä‡πâ 8-bit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î VRAM
        trust_remote_code=True,
    )
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model with 8-bit: {e}")
    # ‡∏•‡∏≠‡∏á‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥
    try:
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            trust_remote_code=True,
        )
        print("Model loaded successfully (without 8-bit)")
    except Exception as e2:
        print(f"Error loading model: {e2}")
        raise e2

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ LoRA adapter
try:
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen models
    target_modules = ["q_proj", "v_proj", "k_proj", "o_proj"]
    if "DialoGPT" in model_name:
        # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DialoGPT
        target_modules = ["c_attn", "c_proj"]
    
    peft_config = LoraConfig(
        r=16, 
        lora_alpha=32,
        target_modules=target_modules,  # ‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏° model architecture
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, peft_config)
    print("LoRA adapter configured successfully")
except Exception as e:
    print(f"Error configuring LoRA: {e}")
    # ‡πÉ‡∏ä‡πâ target_modules ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
    peft_config = LoraConfig(
        r=8, 
        lora_alpha=16,
        target_modules=["attn", "mlp"],  # ‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, peft_config)
    print("LoRA adapter configured with generic targets")

# ‡πÇ‡∏´‡∏•‡∏î‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
try:
    print(f"Loading dataset: {dataset_name}")
    # ‡πÉ‡∏ä‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    train_dataset = load_dataset(dataset_name, split="train[:5%]")  # ‡∏•‡∏î‡πÄ‡∏õ‡πá‡∏ô 5% ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    val_dataset = load_dataset(dataset_name, split="validation[:100]")  # ‡πÉ‡∏ä‡πâ validation set ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á
    print(f"Training dataset loaded: {len(train_dataset)} samples")
    print(f"Validation dataset loaded: {len(val_dataset)} samples")
except Exception as e:
    print(f"Error loading dataset {dataset_name}: {e}")
    # ‡πÉ‡∏ä‡πâ dummy Thai dataset ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    print("Creating dummy Thai dataset for testing...")
    from datasets import Dataset
    dummy_data = {
        "body": [
            "‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡πÑ‡∏î‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÇ‡∏£‡∏Ñ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå",
            "‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏™‡∏á‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡∏° ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏Å‡πä‡∏≤‡∏ã‡πÄ‡∏£‡∏∑‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏à‡∏Å",
            "‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÉ‡∏´‡∏°‡πà‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏ï‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î‡πÑ‡∏î‡πâ‡∏≠‡∏µ‡∏Å‡∏î‡πâ‡∏ß‡∏¢"
        ] * 4,  # 12 samples total
        "summary": [
            "‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏û‡∏±‡∏í‡∏ô‡∏≤ AI ‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÇ‡∏£‡∏Ñ‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å",
            "‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Å‡πä‡∏≤‡∏ã‡πÄ‡∏£‡∏∑‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏à‡∏Å",
            "‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏à‡∏¥‡∏ï‡πÉ‡∏à ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î"
        ] * 4
    }
    train_dataset = Dataset.from_dict(dummy_data)
    val_dataset = Dataset.from_dict(dummy_data)

def preprocess(sample):
    # ‡πÉ‡∏ä‡πâ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á pythainlp/thaisum dataset
    if "body" in sample and "summary" in sample:
        # ‡πÉ‡∏ä‡πâ body (‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡πà‡∏≤‡∏ß) ‡πÄ‡∏õ‡πá‡∏ô article ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ
        article_text = sample["body"]
        summary_text = sample["summary"]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
        prompt = f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:\n\n{article_text}\n\n‡∏™‡∏£‡∏∏‡∏õ:"
        full_text = prompt + summary_text + tokenizer.eos_token
        return {"text": full_text}
    elif "article" in sample and "summary" in sample:
        # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö format ‡πÄ‡∏Å‡πà‡∏≤
        prompt = f"‡∏™‡∏£‡∏∏‡∏õ‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ö‡∏ó‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢:\n{sample['article']}\n‡∏™‡∏£‡∏∏‡∏õ:"
        full_text = prompt + sample["summary"] + tokenizer.eos_token
        return {"text": full_text}
    else:
        # Fallback ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö DialoGPT ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô
        full_text = f"Human: Hello\nBot: Hi there!{tokenizer.eos_token}"
        return {"text": full_text}

train_dataset = train_dataset.map(preprocess)
val_dataset = val_dataset.map(preprocess)
print(f"Training dataset preprocessed: {len(train_dataset)} samples")
print(f"Validation dataset preprocessed: {len(val_dataset)} samples")

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô
output_dir = "./qwen_thai_lora" if "Qwen" in model_name else "./dialogpt_lora"

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU ‡πÅ‡∏•‡∏∞ precision
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
use_fp16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7
use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8

print(f"Device: {device}")
print(f"FP16 support: {use_fp16}")
print(f"BF16 support: {use_bf16}")

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE scores
def compute_rouge_scores(predictions, references):
    """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE scores ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô summarization"""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)
    
    rouge1_scores = []
    rouge2_scores = []
    rougeL_scores = []
    
    for pred, ref in zip(predictions, references):
        # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        pred_clean = pred.strip() if pred else ""
        ref_clean = ref.strip() if ref else ""
        
        if pred_clean and ref_clean:
            scores = scorer.score(ref_clean, pred_clean)
            rouge1_scores.append(scores['rouge1'].fmeasure)
            rouge2_scores.append(scores['rouge2'].fmeasure)
            rougeL_scores.append(scores['rougeL'].fmeasure)
        else:
            rouge1_scores.append(0.0)
            rouge2_scores.append(0.0)
            rougeL_scores.append(0.0)
    
    return {
        'rouge1': np.mean(rouge1_scores),
        'rouge2': np.mean(rouge2_scores),
        'rougeL': np.mean(rougeL_scores)
    }

def compute_metrics(eval_preds):
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ‡πÉ‡∏ô training loop"""
    # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SFTTrainer ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏ó‡∏≥ evaluation ‡πÅ‡∏¢‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏Å
    return {}

def evaluate_model_rouge(model, tokenizer, eval_samples):
    """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ ROUGE scores"""
    print("Generating predictions for ROUGE evaluation...")
    
    predictions = []
    references = []
    
    model.eval()
    with torch.no_grad():
        for i, sample in enumerate(eval_samples):
            if i % 10 == 0:
                print(f"Processing sample {i+1}/{len(eval_samples)}")
            
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
            if "body" in sample and "summary" in sample:
                article = sample["body"]
                reference = sample["summary"]
                prompt = f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:\n\n{article}\n\n‡∏™‡∏£‡∏∏‡∏õ:"
            else:
                continue
                
            # Tokenize ‡πÅ‡∏•‡∏∞ generate
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=400)
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            try:
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=tokenizer.eos_token_id,
                    no_repeat_ngram_size=2
                )
                
                # Decode ‡πÅ‡∏•‡∏∞‡πÅ‡∏¢‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô summary
                generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
                if "‡∏™‡∏£‡∏∏‡∏õ:" in generated:
                    prediction = generated.split("‡∏™‡∏£‡∏∏‡∏õ:")[-1].strip()
                else:
                    prediction = generated.strip()
                
                predictions.append(prediction)
                references.append(reference)
                
            except Exception as e:
                print(f"Error generating for sample {i}: {e}")
                predictions.append("")
                references.append(reference)
    
    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE scores
    if predictions and references:
        rouge_results = compute_rouge_scores(predictions, references)
        
        print("\n" + "="*60)
        print("üîç ROUGE Evaluation Results:")
        print("="*60)
        print(f"ROUGE-1: {rouge_results['rouge1']:.4f}")
        print(f"ROUGE-2: {rouge_results['rouge2']:.4f}")
        print(f"ROUGE-L: {rouge_results['rougeL']:.4f}")
        print("="*60)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        with open(f"{output_dir}/rouge_results.json", "w", encoding="utf-8") as f:
            json.dump({
                "rouge_scores": rouge_results,
                "num_samples": len(predictions),
                "sample_predictions": predictions[:5],  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 5 ‡∏Ç‡πâ‡∏≠‡πÅ‡∏£‡∏Å
                "sample_references": references[:5]
            }, f, ensure_ascii=False, indent=2)
        
        return rouge_results
    else:
        print("No predictions generated for evaluation")
        return None

training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=1,  # ‡∏•‡∏î batch size ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
    per_device_eval_batch_size=1,   # ‡∏•‡∏î eval batch size
    gradient_accumulation_steps=8,  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡∏î‡πÄ‡∏ä‡∏¢ batch size ‡∏ó‡∏µ‡πà‡πÄ‡∏•‡πá‡∏Å‡∏•‡∏á
    num_train_epochs=2,
    learning_rate=2e-4,
    warmup_steps=50,
    fp16=use_fp16 and not use_bf16,
    bf16=use_bf16,
    logging_steps=50,    # logging ‡πÑ‡∏°‡πà‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
    eval_steps=200,      # evaluate ‡∏ô‡πâ‡∏≠‡∏¢‡∏•‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
    eval_strategy="steps",
    save_strategy="steps",
    save_steps=200,
    save_total_limit=1,  # ‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÅ‡∏Ñ‡πà 1 checkpoint
    load_best_model_at_end=False,  # ‡∏õ‡∏¥‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
    remove_unused_columns=False,
    report_to=None,
    dataloader_num_workers=0,
    max_steps=300,  # ‡∏•‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô steps
    dataloader_pin_memory=False,  # ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory
    eval_accumulation_steps=1,  # ‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î memory ‡∏ï‡∏≠‡∏ô evaluation
)

# ‡πÄ‡∏ó‡∏£‡∏ô‡∏î‡πâ‡∏ß‡∏¢ SFTTrainer (supervised fine-tune)
try:
    print("Creating SFTTrainer...")
    # ‡πÉ‡∏ä‡πâ API ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö TRL v0.23.1
    trainer = SFTTrainer(
        model=model,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,  # ‡πÄ‡∏û‡∏¥‡πà‡∏° validation dataset
        args=training_args,
        compute_metrics=compute_metrics,
    )
    
    print("Starting training...")
    trainer.train()
    
    print("Evaluating model with ROUGE scores...")
    # ‡∏ó‡∏≥ evaluation ‡∏ö‡∏ô validation set
    evaluate_model_rouge(model, tokenizer, val_dataset[:20])  # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö 20 samples ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß
    
    print("Saving model...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"Model saved to {output_dir}")
    
except Exception as e:
    print(f"Training error: {e}")
    import traceback
    traceback.print_exc()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏°‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î
    try:
        print("Attempting to save model anyway...")
        model.save_pretrained(output_dir + "_partial")
        tokenizer.save_pretrained(output_dir + "_partial")
        print(f"Partial model saved to {output_dir}_partial")
    except Exception as save_e:
        print(f"Could not save model: {save_e}")
