#!/usr/bin/env python3
"""
Merge LoRA adapter with base model for vLLM hosting
This script merges your Thai LoRA adapter with the base Qwen model
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import sys
from pathlib import Path

def merge_lora_model():
    """Merge LoRA adapter with base model and save the merged model"""
    
    base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    lora_model_path = "./models/qwen_thai_lora"
    merged_model_path = "./models/qwen_thai_merged"
    
    # Check if LoRA model exists
    if not Path(lora_model_path).exists():
        print("âŒ Thai LoRA model not found at ./qwen_thai_lora")
        print("Please run finetune_quen3_lora.py first to create the model")
        return 1
    
    # Check if merged model already exists
    if Path(merged_model_path).exists():
        response = input(f"âš ï¸  Merged model already exists at {merged_model_path}. Overwrite? (y/N): ")
        if response.lower() != 'y':
            print("âŒ Cancelled by user")
            return 1
        
        # Remove existing directory
        import shutil
        shutil.rmtree(merged_model_path)
    
    print("ğŸ”„ Merging LoRA adapter with base model...")
    print(f"ğŸ“ Base model: {base_model_name}")
    print(f"ğŸ“ LoRA adapter: {lora_model_path}")
    print(f"ğŸ“ Output: {merged_model_path}")
    
    try:
        # Load tokenizer
        print("ğŸ“ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
        
        # Load base model
        print("ğŸ¤– Loading base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # Load LoRA adapter
        print("ğŸ”§ Loading LoRA adapter...")
        model = PeftModel.from_pretrained(base_model, lora_model_path)
        
        # Merge LoRA weights into base model
        print("âš¡ Merging LoRA weights...")
        merged_model = model.merge_and_unload()
        
        # Save merged model
        print("ğŸ’¾ Saving merged model...")
        merged_model.save_pretrained(merged_model_path)
        tokenizer.save_pretrained(merged_model_path)
        
        print("âœ… Model merged successfully!")
        print(f"ğŸ“ Merged model saved to: {merged_model_path}")
        print("\nğŸš€ Now you can use vLLM with the merged model:")
        print(f"   ./llm-env/bin/python -m vllm.entrypoints.openai.api_server \\")
        print(f"     --model {merged_model_path} \\")
        print(f"     --host 0.0.0.0 --port 8000 \\")
        print(f"     --served-model-name thai-qwen-merged")
        
        return 0
        
    except Exception as e:
        print(f"âŒ Error merging model: {e}")
        import traceback
        traceback.print_exc()
        return 1

def main():
    """Main function"""
    print("ğŸ”„ Thai LoRA Model Merger")
    print("This script merges your LoRA adapter with the base model for vLLM hosting")
    print("-" * 60)
    
    # Check if we have GPU
    if torch.cuda.is_available():
        print(f"ğŸ® GPU detected: {torch.cuda.get_device_name()}")
    else:
        print("âš ï¸  No GPU detected. Merging will be slower on CPU.")
    
    # Estimate memory requirement
    print("ğŸ’¾ Memory requirement: ~3-4GB GPU memory or 8GB RAM")
    
    confirm = input("\nğŸš€ Proceed with model merging? (y/N): ")
    if confirm.lower() != 'y':
        print("âŒ Cancelled by user")
        return 1
    
    return merge_lora_model()

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)