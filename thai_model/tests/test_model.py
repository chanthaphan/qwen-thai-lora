#!/usr/bin/env python3
"""
Test script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà Fine-tune ‡πÅ‡∏•‡πâ‡∏ß
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

def test_thai_summarization():
    # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞ tokenizer
    base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    lora_model_path = "./models/qwen_thai_lora"
    
    print("Loading base model and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print("Loading LoRA adapter...")
    model = PeftModel.from_pretrained(base_model, lora_model_path)
    model.eval()
    
    # ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    test_articles = [
        """‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏à‡∏≤‡∏Å‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡∏ä‡∏±‡πâ‡∏ô‡∏ô‡∏≥‡πÑ‡∏î‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•„Ç∏‡∏µ‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÇ‡∏£‡∏Ñ‡∏°‡∏∞‡πÄ‡∏£‡πá‡∏á‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏ñ‡πà‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏ó‡∏¢‡πå ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏∞‡∏´‡∏ô‡∏±‡∏Å‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥‡πÑ‡∏î‡πâ‡∏ñ‡∏∂‡∏á 95% ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÅ‡∏ö‡∏ö‡∏î‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡∏∂‡∏á 15% ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ‡∏£‡∏∞‡∏ö‡∏ö‡∏¢‡∏±‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÑ‡∏î‡πâ‡πÄ‡∏£‡πá‡∏ß‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°‡∏ñ‡∏∂‡∏á 3 ‡πÄ‡∏ó‡πà‡∏≤""",
        
        """‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡πÑ‡∏î‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡πÄ‡∏™‡∏£‡∏¥‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏ó‡∏î‡πÅ‡∏ó‡∏ô ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡πÅ‡∏™‡∏á‡∏≠‡∏≤‡∏ó‡∏¥‡∏ï‡∏¢‡πå‡πÅ‡∏•‡∏∞‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏•‡∏° ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏Å‡πä‡∏≤‡∏ã‡πÄ‡∏£‡∏∑‡∏≠‡∏ô‡∏Å‡∏£‡∏∞‡∏à‡∏Å‡∏•‡∏á 30% ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏õ‡∏µ 2030 ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ‡∏™‡∏∞‡∏≠‡∏≤‡∏î ‡∏£‡∏±‡∏ê‡∏ö‡∏≤‡∏•‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ó‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏µ‡πÅ‡∏Å‡πà‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∏‡∏ô‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏•‡∏±‡∏á‡∏á‡∏≤‡∏ô‡∏™‡∏∞‡∏≠‡∏≤‡∏î""",
        
        """‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà‡∏û‡∏ö‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏™‡∏°‡πà‡∏≥‡πÄ‡∏™‡∏°‡∏≠‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 30 ‡∏ô‡∏≤‡∏ó‡∏µ‡∏ï‡πà‡∏≠‡∏ß‡∏±‡∏ô ‡πÑ‡∏°‡πà‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡πÅ‡∏ï‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏£‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏¢‡πÅ‡∏Ç‡πá‡∏á‡πÅ‡∏£‡∏á‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏à‡∏¥‡∏ï‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏£‡∏µ‡∏¢‡∏î ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡∏¥‡∏ï‡∏Å‡∏Å‡∏±‡∏á‡∏ß‡∏• ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ô‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏ö ‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Å‡∏¥‡∏à‡∏Å‡∏£‡∏£‡∏°‡∏ó‡∏µ‡πà‡∏ä‡∏≠‡∏ö ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏î‡∏¥‡∏ô ‡∏ß‡∏¥‡πà‡∏á ‡∏ß‡πà‡∏≤‡∏¢‡∏ô‡πâ‡∏≥ ‡∏´‡∏£‡∏∑‡∏≠‡πÇ‡∏¢‡∏Ñ‡∏∞"""
    ]
    
    print("\n" + "="*60)
    print("‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢")
    print("="*60)
    
    for i, article in enumerate(test_articles, 1):
        print(f"\nüì∞ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ó‡∏µ‡πà {i}:")
        print("-" * 40)
        print(f"üìÑ ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:\n{article}")
        print("\n" + "." * 40)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt
        prompt = f"‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πà‡∏≤‡∏ß‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:\n\n{article}\n\n‡∏™‡∏£‡∏∏‡∏õ:"
        
        # Tokenize
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=450)
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3
            )
        
        # Decode
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # ‡πÅ‡∏¢‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
        try:
            summary = generated.split("‡∏™‡∏£‡∏∏‡∏õ:")[-1].strip()
        except:
            summary = generated
            
        print(f"üîç ‡∏™‡∏£‡∏∏‡∏õ: {summary}")
        print()

if __name__ == "__main__":
    test_thai_summarization()