# ğŸ‡¹ğŸ‡­ Thai Language Model Project

A comprehensive Python project for fine-tuning and deploying Thai language models using state-of-the-art techniques including LoRA (Low-Rank Adaptation) and modern hosting solutions.

## ğŸš€ Quick Start

```bash
# Clone and setup
git clone <your-repo-url>
cd project
./setup.sh

# Train Thai model
./manage.sh train

# Start web interface
./manage.sh host-gui

# Or start API server
./manage.sh host-api
```

## ğŸ“ Project Structure

```
project/
â”œâ”€â”€ ğŸ“š src/                     # Source code
â”‚   â”œâ”€â”€ ğŸ§  training/            # Model training scripts
â”‚   â”‚   â”œâ”€â”€ finetune_thai_model.py
â”‚   â”‚   â””â”€â”€ merge_lora_model.py
â”‚   â”œâ”€â”€ ğŸŒ hosting/             # Model hosting servers
â”‚   â”‚   â”œâ”€â”€ fastapi_server.py
â”‚   â”‚   â””â”€â”€ host_thai_model.py
â”‚   â”œâ”€â”€ ğŸ–¥ï¸ interfaces/          # User interfaces
â”‚   â”‚   â”œâ”€â”€ gradio_gui.py
â”‚   â”‚   â”œâ”€â”€ ollama_chat.py
â”‚   â”‚   â””â”€â”€ vllm_chat.py
â”‚   â”œâ”€â”€ ğŸ§ª testing/             # Test scripts
â”‚   â”‚   â”œâ”€â”€ test_model.py
â”‚   â”‚   â””â”€â”€ test_simple.py
â”‚   â””â”€â”€ ğŸ”§ utils/               # Utility functions
â”œâ”€â”€ ğŸ—ï¸ models/                  # Trained models
â”‚   â””â”€â”€ qwen_thai_lora/         # Thai LoRA model
â”œâ”€â”€ ğŸ“‹ config/                  # Configuration files
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ ğŸ³ deployment/              # Deployment files
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ ğŸ“– docs/                    # Documentation
â”œâ”€â”€ ğŸ› ï¸ scripts/                 # Utility scripts
â”œâ”€â”€ ğŸ“Š logs/                    # Training logs
â”œâ”€â”€ ğŸ’¾ data/                    # Dataset cache
â”œâ”€â”€ manage.sh                   # ğŸ¯ Project manager
â””â”€â”€ setup.sh                   # âš¡ Quick setup
```

## ğŸ¯ Project Manager

Use the `./manage.sh` script for all common tasks:

```bash
# Setup and environment
./manage.sh setup          # Setup project environment
./manage.sh status         # Show project status
./manage.sh clean          # Clean temporary files

# Training and testing
./manage.sh train          # Train Thai model
./manage.sh test           # Run comprehensive tests
./manage.sh test-simple    # Run quick model test
./manage.sh merge-model    # Merge LoRA with base model

# Hosting and interfaces
./manage.sh host-gui       # Start Gradio web interface
./manage.sh host-api       # Start FastAPI server
./manage.sh chat-ollama    # Start Ollama chat app
./manage.sh chat-vllm      # Start vLLM chat app

# Docker deployment
./manage.sh docker-build   # Build Docker image
./manage.sh docker-run     # Run Docker container

./manage.sh help           # Show all commands
```

## ğŸ§  Features

### ğŸ“ Training Capabilities
- **LoRA Fine-tuning**: Parameter-efficient training with PEFT library
- **Thai Dataset**: pythainlp/thaisum with 350k+ Thai news articles
- **Modern Framework**: Uses TRL SFTTrainer with transformers 4.35+
- **Evaluation Metrics**: ROUGE score evaluation for summarization quality
- **GPU Optimization**: Mixed precision training with accelerate

### ğŸŒ Hosting Options
- **FastAPI Server**: Production-ready API with OpenAI-compatible endpoints
- **Gradio Web UI**: User-friendly web interface for Thai summarization
- **Docker Support**: Containerized deployment with GPU support
- **vLLM Integration**: High-performance inference server option
- **Ollama Chat**: Interactive chat interface for Ollama models

### ğŸ”§ Technical Stack
- **Base Model**: Qwen2.5-1.5B-Instruct
- **Fine-tuning**: PEFT LoRA (rank 16, alpha 32)
- **Framework**: PyTorch + Transformers + TRL
- **Language Processing**: pythainlp for Thai text handling
- **APIs**: FastAPI with automatic OpenAPI documentation
- **Frontend**: Gradio for web interfaces

## ğŸ“Š Training Details

The model is fine-tuned on Thai news summarization using:

- **Dataset**: pythainlp/thaisum (Thai news articles with summaries)
- **Architecture**: Qwen2.5-1.5B-Instruct with LoRA adapters
- **Training**: 3 epochs with gradient accumulation
- **Optimization**: AdamW optimizer with learning rate scheduling
- **Evaluation**: ROUGE-1, ROUGE-2, ROUGE-L metrics

### ğŸ¯ Performance
- **Model Size**: ~1.5B parameters (base) + ~8M (LoRA)
- **Training Time**: ~2-3 hours on RTX 4090
- **Memory Usage**: ~6-8GB VRAM during training
- **Inference Speed**: ~10-15 tokens/second

## ğŸš€ Usage Examples

### Python API
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Load model
base_model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
model = PeftModel.from_pretrained(base_model, "models/qwen_thai_lora")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")

# Generate summary
prompt = "à¸ªà¸£à¸¸à¸›à¸‚à¹ˆà¸²à¸§à¸™à¸µà¹‰: [Thai news article]"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_length=200)
summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
```

### FastAPI Server
```bash
# Start server
./manage.sh host-api

# Use API
curl -X POST "http://localhost:8001/v1/chat/completions" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "thai-model",
    "messages": [{"role": "user", "content": "à¸ªà¸£à¸¸à¸›à¸‚à¹ˆà¸²à¸§à¸™à¸µà¹‰: ..."}]
  }'
```

### Web Interface
```bash
# Start Gradio interface
./manage.sh host-gui

# Open browser to http://localhost:7860
```

## ğŸ› ï¸ Development

### Environment Setup
```bash
# Manual setup
python3 -m venv llm-env
source llm-env/bin/activate
pip install -r config/requirements.txt

# Or use quick setup
./setup.sh
```

### Adding New Features
1. **Training**: Add scripts to `src/training/`
2. **Hosting**: Add servers to `src/hosting/`
3. **Interfaces**: Add UIs to `src/interfaces/`
4. **Tests**: Add tests to `src/testing/`
5. **Utils**: Add utilities to `src/utils/`

### Testing
```bash
# Quick test
./manage.sh test-simple

# Full test suite
./manage.sh test

# Manual testing
source llm-env/bin/activate
python src/testing/test_model.py
```

## ğŸ³ Docker Deployment

```bash
# Build and run
./manage.sh docker-build
./manage.sh docker-run

# Or use docker-compose
cd deployment
docker-compose up -d
```

## ğŸ“ˆ Performance Tuning

### Training Optimization
- Adjust LoRA rank/alpha in training script
- Modify batch size based on GPU memory
- Experiment with different learning rates
- Use gradient checkpointing for larger models

### Inference Optimization
- Use model quantization for smaller memory footprint
- Implement response caching for common queries
- Deploy on multiple GPUs for higher throughput
- Use vLLM for production-scale inference

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make changes and test: `./manage.sh test`
4. Commit changes: `git commit -am 'Add feature'`
5. Push to branch: `git push origin feature-name`
6. Submit a pull request

## ğŸ“ License

MIT License - see LICENSE file for details.

## ğŸ™ Acknowledgments

- **Qwen Team**: For the excellent base model
- **Hugging Face**: For transformers and PEFT libraries
- **Thai NLP Community**: For pythainlp and Thai datasets
- **Meta**: For LoRA technique and research

## ğŸ“ Support

- ğŸ“§ Issues: Use GitHub Issues for bug reports
- ğŸ’¬ Discussions: Use GitHub Discussions for questions
- ğŸ“š Documentation: Check `docs/` folder for detailed guides